# Load libraries
library(dplyr)
library(ggplot2)
library(e1071)
library(caret) ## Can be used for one-hot encoding
library(pROC)
library(rpart) # Create decision tree classification model
library(randomForest) # Random forest package

# Load the data
heart <- read.csv("heart.csv", header=TRUE)
head(heart)

# Check the data structure
str(heart)
summary(heart)
table(is.na(heart))

colnames(heart)

# EDA: Explore data trends
# 1. age
ggplot(heart, aes(x=age, fill=factor(age))) +
  geom_bar()

# 2. sex
ggplot(heart, aes(x=sex, fill=factor(target))) +
  geom_bar(position = "dodge") +
  labs(title="Sex Distribution",
       x="Sex",
       y="Counts",
       fill="Target")

# 3. cp
ggplot(heart, aes(x=cp, fill=factor(target))) +
  geom_bar(position = "dodge")

# trestbps
ggplot(heart, aes(x=trestbps, fill=factor(trestbps))) +
  geom_bar(position = "dodge")

# Detect outliers using boxplots
# theme_minimal(): Set white background for a clean graph
ggplot(heart, aes(x=target, y=chol, group=target, fill=factor(target))) +
  geom_boxplot() +
  labs(title="Boxplot - Chol",
       x="Target",
       y="Chol",
       fill="Target") +
  theme_minimal()

# Outlier removal function
remove_or <- function (x, lower_percentile=0.05,upper_percentile=0.95){
  lower_limit <- quantile(x, lower_percentile)
  upper_limit <- quantile(x, upper_percentile)
  return(x[x >=lower_limit & x<= upper_limit])
}

# Check for outliers in each numeric column
numeric_columns <- sapply(heart, is.numeric)
outlier_indices <- lapply(heart[, numeric_columns], function(col) {
  remove_or(col)
})

# Remove common rows across all columns
common_rows <- Reduce(intersect, lapply(outlier_indices, rownames))
heart_cleaned <- heart[common_rows, ]

# Confirm target is categorical, perform one-hot encoding using caret library
heart_encoded <- dummyVars(~., data=heart)
data_transformed <- data.frame(predict(heart_encoded, newdata=heart))
str(data_transformed)

# Normalize data for machine learning model performance improvement
preprocessed_heart <- preProcess(data_transformed, method=c("scale"))
normalized_data <- predict(preprocessed_heart, newdata=data_transformed)
print(normalized_data)

# Split data into train and test sets
index <- sample(1:nrow(normalized_data), 0.7*nrow(normalized_data))
train_data <- normalized_data[index, ]
test_data <- normalized_data[-index, ]
test_data1 <- test_data[, -14]  # Exclude target variable from test data
nrow(test_data)  # Check count
nrow(train_data)  # Check count
ncol(test_data)
head(test_data1)
head(test_data)

# Create classification models: 1. Decision Tree
model_dc <- rpart(target ~ ., data=train_data, method="class")

# Visualize the model
plot(model_dc)
text(model_dc, use.n=TRUE)

# Predict and evaluate
prediction_dc <- predict(model_dc, newdata=test_data1, type="class")
pr_dc <- ifelse(as.numeric(prediction_dc)>1.0, 1, 0)
true_dc <- ifelse(test_data$target>1.0, 1, 0)

length(prediction_dc)

# Confusion matrix
cm_dc <- table(Actual=true_dc, Predicted=pr_dc)
print(cm_dc)

# Evaluate metrics
TP <- cm_dc[1, 1]
FP <- cm_dc[2, 1]
TN <- cm_dc[2, 2]
FN <- cm_dc[1, 2]

accuracy_dc <- (TP+TN)/(sum(cm_dc))
precision_dc <- TP / (TP+FP)
recall_dc <- TP / (TP + FN) 
F1_dc <- (2* (precision_dc*recall_dc)) /(precision_dc+recall_dc)

results_dc <- data.frame(accuracy_dc, precision_dc, recall_dc, F1_dc)
print(results_dc)

## Continue with Random Forest for classification and prediction
# Create the model
model_rf <- randomForest(target ~ ., data=train_data, ntree=500, importance=TRUE)
print(model_rf)

# Predict using the model
prediction_rf <- predict(model_rf, newdata=test_data1)
pr_rf <- ifelse(prediction_rf>1.0, 1, 0)
length(pr_rf)

# Confusion matrix
cm_rf <- table(Actual=true_dc, Predicted=pr_rf)
print(cm_rf)

# Calculate metrics
TP <- cm_rf[1, 1]
FN <- cm_rf[1, 2]
FP <- cm_rf[2, 1]
TN <- cm_rf[2, 2]

ac_rf <- (TP+TN)/ sum(cm_rf)
recall_rf <- (TP)/(TP+FP)
precision_rf <- TP / (TP+FN)
F1_rf <- (2* (precision_rf*recall_rf)) /(precision_rf+recall_rf)

result_rf <- data.frame(ac_rf, recall_rf, precision_rf, F1_rf)
print(result_rf)

## Apply Logistic Regression (GLM)
train_data$target <- ifelse(train_data$target>1.0, 1, 0)
model_glm <- glm(target ~ ., data=train_data, family=binomial)
prediction_glm <- predict(model_glm, newdata=test_data1, type="response")
pr_glm <- ifelse(prediction_glm>0.5, 1, 0)

nrow(test_data1)
length(pr_glm)

# Confusion matrix
cm_glm <- table(Actual=true_dc, Predicted=pr_glm)
print(cm_glm)

# Calculate metrics
TP <- cm_glm[1, 1]
FN <- cm_glm[1, 2]
FP <- cm_glm[2, 1]
TN <- cm_glm[2, 2]

accuracy_glm <- (TP+TN) / sum(cm_glm)
precision_glm <- (TP) / (TP+FP)
recall_glm <- TP/(TP+FN)
F1_glm <- (2* (precision_glm*recall_glm)) /(precision_gbm+recall_glm)

result_glm <- data.frame(accuracy_glm, precision_glm, recall_glm, F1_glm)
print(result_glm)

### Apply Gradient Boosting

model_gbm <- gbm(target ~ ., data=train_data, distribution="bernoulli", 
                 n.trees=100, interaction.depth=4)
prediction_gbm <- predict(model_gbm, newdata=test_data1, 
                          n.trees=100, interaction.depth=4)
pr_gbm <- ifelse(prediction_gbm>0.5, 1, 0)

# Confusion matrix
cm_gbm <- table(Actual=true_dc, Predicted=pr_gbm)
print(cm_gbm)

# Calculate metrics
TP <- cm_gbm[1, 1]
FN <- cm_gbm[1, 2]
FP <- cm_gbm[2, 1]
TN <- cm_gbm[2, 2]

accuracy_gbm <- (TP+TN) / sum(cm_glm)
precision_gbm <- (TP) / (TP+FP)
recall_gbm <- TP/(TP+FN)
F1_gbm <- (2* (precision_gbm*recall_gbm)) /(precision_gbm+recall_gbm)

## Compare Recall Only
total_recall <- data.frame(results_dc$recall_dc, result_rf$recall_rf, 
                           result_glm$recall_glm, result_gbm$recall_gbm)
ggplot(total_recall)+
  geom_bar()

total_recall <- data.frame(
  model <- c("DC", "RF", "GLM", "GBM"),
  recall<-c(results_dc$recall_dc, result_rf$recall_rf, 
            result_glm$recall_glm, result_gbm$recall_gbm)
)

colnames(total_recall) <- c("method","recall")

ggplot(total_recall, aes(x=method, y=recall, fill=model))+
  geom_bar(stat = "identity")+
  theme_minimal()


ggplot(total_recall, aes(x = model, y = recall, fill = model)) +
  geom_bar(stat = "identity") +  
  labs(title = "Recall by Model", x = "Model", y = "Recall") +
  geom_text(aes(label=sprintf("%.2f", recall))) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


## Compare Accuracy
total_accuracy <- data.frame(results_dc$accuracy_dc, result_rf$ac_rf, 
                             result_glm$accuracy_glm, result_gbm$accuracy_gbm)

total_recall <- data.frame(
  model <- c("DC", "RF", "GLM", "GBM"),
  recall<-c(results_dc$accuracy_dc, result_rf$ac_rf, 
            result_glm$accuracy_glm, result_gbm$accuracy_gbm)
)

colnames(total_recall) <- c("method", "accuracy")

ggplot(total_recall, aes(x=method, y=accuracy, fill=method))+
  geom_bar(stat="identity")+
  geom_text(aes(label=sprintf("%.2f", accuracy)))


### Run the same process with outlier-removed data
# Calculate Z-scores
z_scores_trestbps <- scale(heart$trestbps)
z_scores_chol <- scale(heart$chol)
z_scores_thalach <- scale(heart$thalach)
z_scores_oldpeak <- scale(heart$oldpeak)

# Z-score based outlier removal
threshold <- 3
outliers <- which(abs(z_scores_trestbps) > threshold)
outliers1 <- which(abs(z_scores_chol) > threshold)
outliers2 <- which(abs(z_scores_thalach) > threshold)
outliers3 <- which(abs(z_scores_oldpeak) > threshold)
heart <- heart[-outliers, ]
heart <- heart[-outliers1, ]
heart <- heart[-outliers2, ]
heart <- heart[-outliers3, ]

nrow(heart)  # Removed a total of 18 outliers

# Result: Running classification and prediction models with outlier removal
# resulted in different optimal models and improved accuracy.
# Outlier removal is important.
